{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "#network and training\n",
    "NB_EPOCH=200\n",
    "BATCH_SIZE=64\n",
    "VERBOSE=1\n",
    "NB_CLASSES=10 # number of outputs=number of digits\n",
    "OPTIMIZER=SGD() #SGD optimizer , explained later in this chapter\n",
    "N_HIDDEN=128\n",
    "VALIDATION_SPLIT=0.2 #how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAADOCAYAAACpdxJrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD5YA0MMS+fWYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110819910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "#plot first six training images\n",
    "fig=plt.figure(figsize=(20,20))\n",
    "for i in range(6):\n",
    "    #ax=fig.add_subplot(1,6,i+1,xtics=[],yticks=[])\n",
    "    ax=fig.add_subplot(1,6,i+1)\n",
    "    ax.imshow(X_train[i],cmap='gray')\n",
    "    ax.set_title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RESHAPED=784\n",
    "#\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train=X_train.reshape(60000,RESHAPED)\n",
    "print('after reshape',X_train.shape)\n",
    "X_test=X_test.reshape(10000,RESHAPED)\n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32')\n",
    "#normalize\n",
    "#\n",
    "X_train/=255\n",
    "X_test/=255\n",
    "print(X_train.shape,X_train.shape[0],'train samples')\n",
    "print(X_train.shape,X_test.shape[0],'test samples')\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "y_train=np_utils.to_categorical(y_train,NB_CLASSES)\n",
    "y_test=np_utils.to_categorical(y_test,NB_CLASSES)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#10 outputs\n",
    "# final stage is softmax\n",
    "model=Sequential()\n",
    "model.add(Dense(NB_CLASSES,input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 1.0784 - acc: 0.7514 - val_loss: 0.6576 - val_acc: 0.8536\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.6082 - acc: 0.8553 - val_loss: 0.5100 - val_acc: 0.8768\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.5129 - acc: 0.8703 - val_loss: 0.4517 - val_acc: 0.8855\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.4665 - acc: 0.8783 - val_loss: 0.4191 - val_acc: 0.8922\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.4379 - acc: 0.8837 - val_loss: 0.3980 - val_acc: 0.8952\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.4180 - acc: 0.8877 - val_loss: 0.3827 - val_acc: 0.9000\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.4031 - acc: 0.8910 - val_loss: 0.3713 - val_acc: 0.9003\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3914 - acc: 0.8929 - val_loss: 0.3619 - val_acc: 0.9030\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3819 - acc: 0.8958 - val_loss: 0.3548 - val_acc: 0.9038\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3741 - acc: 0.8973 - val_loss: 0.3483 - val_acc: 0.9049\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3672 - acc: 0.8990 - val_loss: 0.3431 - val_acc: 0.9062\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3615 - acc: 0.9000 - val_loss: 0.3386 - val_acc: 0.9075\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3564 - acc: 0.9011 - val_loss: 0.3346 - val_acc: 0.9083\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3518 - acc: 0.9025 - val_loss: 0.3310 - val_acc: 0.9095\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3478 - acc: 0.9034 - val_loss: 0.3278 - val_acc: 0.9096\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3441 - acc: 0.9046 - val_loss: 0.3249 - val_acc: 0.9103\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3408 - acc: 0.9048 - val_loss: 0.3224 - val_acc: 0.9102\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3378 - acc: 0.9063 - val_loss: 0.3200 - val_acc: 0.9113\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3350 - acc: 0.9063 - val_loss: 0.3177 - val_acc: 0.9118\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3324 - acc: 0.9073 - val_loss: 0.3160 - val_acc: 0.9121\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3300 - acc: 0.9081 - val_loss: 0.3140 - val_acc: 0.9130\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3278 - acc: 0.9089 - val_loss: 0.3122 - val_acc: 0.9135\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3257 - acc: 0.9093 - val_loss: 0.3105 - val_acc: 0.9136\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3239 - acc: 0.9098 - val_loss: 0.3090 - val_acc: 0.9146\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3219 - acc: 0.9105 - val_loss: 0.3077 - val_acc: 0.9153\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3203 - acc: 0.9111 - val_loss: 0.3063 - val_acc: 0.9155\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3185 - acc: 0.9115 - val_loss: 0.3050 - val_acc: 0.9153\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3171 - acc: 0.9124 - val_loss: 0.3042 - val_acc: 0.9152\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3157 - acc: 0.9124 - val_loss: 0.3028 - val_acc: 0.9153\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3142 - acc: 0.9129 - val_loss: 0.3017 - val_acc: 0.9162\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3129 - acc: 0.9134 - val_loss: 0.3008 - val_acc: 0.9159\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3118 - acc: 0.9135 - val_loss: 0.2998 - val_acc: 0.9157\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3104 - acc: 0.9136 - val_loss: 0.2989 - val_acc: 0.9164\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3094 - acc: 0.9139 - val_loss: 0.2980 - val_acc: 0.9167\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3082 - acc: 0.9141 - val_loss: 0.2973 - val_acc: 0.9158\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3072 - acc: 0.9145 - val_loss: 0.2963 - val_acc: 0.9176\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3061 - acc: 0.9142 - val_loss: 0.2956 - val_acc: 0.9170\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3051 - acc: 0.9151 - val_loss: 0.2950 - val_acc: 0.9177\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3043 - acc: 0.9152 - val_loss: 0.2942 - val_acc: 0.9173\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3033 - acc: 0.9156 - val_loss: 0.2940 - val_acc: 0.9177\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3025 - acc: 0.9156 - val_loss: 0.2930 - val_acc: 0.9179\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3016 - acc: 0.9160 - val_loss: 0.2923 - val_acc: 0.9192\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3008 - acc: 0.9164 - val_loss: 0.2919 - val_acc: 0.9181\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3001 - acc: 0.9160 - val_loss: 0.2913 - val_acc: 0.9188\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2993 - acc: 0.9165 - val_loss: 0.2907 - val_acc: 0.9194\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2985 - acc: 0.9170 - val_loss: 0.2902 - val_acc: 0.9190\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2978 - acc: 0.9172 - val_loss: 0.2898 - val_acc: 0.9197\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2971 - acc: 0.9175 - val_loss: 0.2891 - val_acc: 0.9200\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2965 - acc: 0.9175 - val_loss: 0.2887 - val_acc: 0.9197\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2959 - acc: 0.9176 - val_loss: 0.2881 - val_acc: 0.9203\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2952 - acc: 0.9173 - val_loss: 0.2878 - val_acc: 0.9203\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2945 - acc: 0.9179 - val_loss: 0.2874 - val_acc: 0.9199\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2940 - acc: 0.9182 - val_loss: 0.2870 - val_acc: 0.9204\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2934 - acc: 0.9185 - val_loss: 0.2864 - val_acc: 0.9210\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2928 - acc: 0.9183 - val_loss: 0.2861 - val_acc: 0.9207\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2923 - acc: 0.9184 - val_loss: 0.2855 - val_acc: 0.9211\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2917 - acc: 0.9189 - val_loss: 0.2856 - val_acc: 0.9207\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2911 - acc: 0.9191 - val_loss: 0.2852 - val_acc: 0.9212\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2907 - acc: 0.9189 - val_loss: 0.2847 - val_acc: 0.9209\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2902 - acc: 0.9190 - val_loss: 0.2842 - val_acc: 0.9216\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2897 - acc: 0.9188 - val_loss: 0.2839 - val_acc: 0.9214\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2892 - acc: 0.9190 - val_loss: 0.2838 - val_acc: 0.9212\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2887 - acc: 0.9191 - val_loss: 0.2834 - val_acc: 0.9213\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2883 - acc: 0.9196 - val_loss: 0.2829 - val_acc: 0.9217\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2879 - acc: 0.9196 - val_loss: 0.2826 - val_acc: 0.9217\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2873 - acc: 0.9199 - val_loss: 0.2824 - val_acc: 0.9221\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2870 - acc: 0.9197 - val_loss: 0.2820 - val_acc: 0.9213\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2866 - acc: 0.9197 - val_loss: 0.2817 - val_acc: 0.9217\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2861 - acc: 0.9199 - val_loss: 0.2817 - val_acc: 0.9216\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2857 - acc: 0.9201 - val_loss: 0.2813 - val_acc: 0.9216\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2853 - acc: 0.9200 - val_loss: 0.2810 - val_acc: 0.9217\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2850 - acc: 0.9203 - val_loss: 0.2808 - val_acc: 0.9219\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2845 - acc: 0.9208 - val_loss: 0.2808 - val_acc: 0.9220\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2842 - acc: 0.9203 - val_loss: 0.2804 - val_acc: 0.9220\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2838 - acc: 0.9205 - val_loss: 0.2802 - val_acc: 0.9222\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2835 - acc: 0.9210 - val_loss: 0.2801 - val_acc: 0.9224\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2831 - acc: 0.9207 - val_loss: 0.2797 - val_acc: 0.9223\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2828 - acc: 0.9210 - val_loss: 0.2794 - val_acc: 0.9223\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2824 - acc: 0.9213 - val_loss: 0.2793 - val_acc: 0.9225\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2821 - acc: 0.9213 - val_loss: 0.2790 - val_acc: 0.9227\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2818 - acc: 0.9213 - val_loss: 0.2791 - val_acc: 0.9227\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2815 - acc: 0.9215 - val_loss: 0.2787 - val_acc: 0.9226\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2811 - acc: 0.9219 - val_loss: 0.2785 - val_acc: 0.9227\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2808 - acc: 0.9215 - val_loss: 0.2783 - val_acc: 0.9232\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2805 - acc: 0.9218 - val_loss: 0.2781 - val_acc: 0.9224\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2802 - acc: 0.9220 - val_loss: 0.2781 - val_acc: 0.9231\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2799 - acc: 0.9216 - val_loss: 0.2777 - val_acc: 0.9228\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2796 - acc: 0.9220 - val_loss: 0.2775 - val_acc: 0.9228\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2793 - acc: 0.9223 - val_loss: 0.2773 - val_acc: 0.9233\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2790 - acc: 0.9222 - val_loss: 0.2772 - val_acc: 0.9227\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2788 - acc: 0.9222 - val_loss: 0.2770 - val_acc: 0.9233\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2785 - acc: 0.9223 - val_loss: 0.2768 - val_acc: 0.9233\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2783 - acc: 0.9223 - val_loss: 0.2766 - val_acc: 0.9237\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2780 - acc: 0.9227 - val_loss: 0.2765 - val_acc: 0.9236\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2777 - acc: 0.9229 - val_loss: 0.2766 - val_acc: 0.9226\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2775 - acc: 0.9225 - val_loss: 0.2763 - val_acc: 0.9234\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2772 - acc: 0.9229 - val_loss: 0.2761 - val_acc: 0.9235\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2769 - acc: 0.9227 - val_loss: 0.2759 - val_acc: 0.9240\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2767 - acc: 0.9227 - val_loss: 0.2757 - val_acc: 0.9239\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2765 - acc: 0.9229 - val_loss: 0.2758 - val_acc: 0.9238\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2762 - acc: 0.9230 - val_loss: 0.2755 - val_acc: 0.9241\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2760 - acc: 0.9230 - val_loss: 0.2754 - val_acc: 0.9242\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2757 - acc: 0.9229 - val_loss: 0.2753 - val_acc: 0.9236\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2755 - acc: 0.9232 - val_loss: 0.2752 - val_acc: 0.9241\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2753 - acc: 0.9232 - val_loss: 0.2750 - val_acc: 0.9245\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2751 - acc: 0.9233 - val_loss: 0.2748 - val_acc: 0.9242\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2748 - acc: 0.9232 - val_loss: 0.2748 - val_acc: 0.9238\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2746 - acc: 0.9232 - val_loss: 0.2746 - val_acc: 0.9243\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2744 - acc: 0.9235 - val_loss: 0.2747 - val_acc: 0.9237\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2741 - acc: 0.9235 - val_loss: 0.2744 - val_acc: 0.9239\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2740 - acc: 0.9235 - val_loss: 0.2742 - val_acc: 0.9245\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2737 - acc: 0.9237 - val_loss: 0.2743 - val_acc: 0.9233\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2735 - acc: 0.9236 - val_loss: 0.2739 - val_acc: 0.9243\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2733 - acc: 0.9237 - val_loss: 0.2739 - val_acc: 0.9241\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2731 - acc: 0.9238 - val_loss: 0.2741 - val_acc: 0.9244\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2730 - acc: 0.9236 - val_loss: 0.2736 - val_acc: 0.9249\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2727 - acc: 0.9238 - val_loss: 0.2735 - val_acc: 0.9247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2725 - acc: 0.9241 - val_loss: 0.2735 - val_acc: 0.9249\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2724 - acc: 0.9238 - val_loss: 0.2733 - val_acc: 0.9247\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2722 - acc: 0.9242 - val_loss: 0.2733 - val_acc: 0.9247\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2720 - acc: 0.9242 - val_loss: 0.2732 - val_acc: 0.9251\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2717 - acc: 0.9243 - val_loss: 0.2732 - val_acc: 0.9250\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2716 - acc: 0.9243 - val_loss: 0.2731 - val_acc: 0.9244\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2714 - acc: 0.9244 - val_loss: 0.2731 - val_acc: 0.9248\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2712 - acc: 0.9243 - val_loss: 0.2729 - val_acc: 0.9243\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2710 - acc: 0.9247 - val_loss: 0.2728 - val_acc: 0.9253\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2709 - acc: 0.9243 - val_loss: 0.2728 - val_acc: 0.9248\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2707 - acc: 0.9242 - val_loss: 0.2725 - val_acc: 0.9252\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2705 - acc: 0.9247 - val_loss: 0.2724 - val_acc: 0.9245\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2704 - acc: 0.9249 - val_loss: 0.2721 - val_acc: 0.9249\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2701 - acc: 0.9248 - val_loss: 0.2726 - val_acc: 0.9243\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2700 - acc: 0.9247 - val_loss: 0.2723 - val_acc: 0.9242\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2698 - acc: 0.9249 - val_loss: 0.2721 - val_acc: 0.9250\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2697 - acc: 0.9248 - val_loss: 0.2722 - val_acc: 0.9247\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2695 - acc: 0.9247 - val_loss: 0.2720 - val_acc: 0.9243\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2693 - acc: 0.9248 - val_loss: 0.2719 - val_acc: 0.9247\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2692 - acc: 0.9249 - val_loss: 0.2719 - val_acc: 0.9253\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2690 - acc: 0.9247 - val_loss: 0.2717 - val_acc: 0.9252\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2689 - acc: 0.9250 - val_loss: 0.2716 - val_acc: 0.9249\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2687 - acc: 0.9250 - val_loss: 0.2714 - val_acc: 0.9247\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2686 - acc: 0.9254 - val_loss: 0.2714 - val_acc: 0.9247\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2684 - acc: 0.9254 - val_loss: 0.2714 - val_acc: 0.9248\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2683 - acc: 0.9249 - val_loss: 0.2712 - val_acc: 0.9252\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2681 - acc: 0.9253 - val_loss: 0.2713 - val_acc: 0.9247\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2680 - acc: 0.9253 - val_loss: 0.2711 - val_acc: 0.9249\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2677 - acc: 0.9254 - val_loss: 0.2712 - val_acc: 0.9247\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2676 - acc: 0.9254 - val_loss: 0.2710 - val_acc: 0.9246\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2675 - acc: 0.9256 - val_loss: 0.2711 - val_acc: 0.9247\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2674 - acc: 0.9255 - val_loss: 0.2709 - val_acc: 0.9249\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2673 - acc: 0.9256 - val_loss: 0.2706 - val_acc: 0.9252\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2671 - acc: 0.9256 - val_loss: 0.2706 - val_acc: 0.9251\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2670 - acc: 0.9256 - val_loss: 0.2706 - val_acc: 0.9253\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.2668 - acc: 0.9257 - val_loss: 0.2705 - val_acc: 0.9247\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2666 - acc: 0.9255 - val_loss: 0.2705 - val_acc: 0.9249\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2665 - acc: 0.9256 - val_loss: 0.2704 - val_acc: 0.9245\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2664 - acc: 0.9256 - val_loss: 0.2704 - val_acc: 0.9253\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2663 - acc: 0.9256 - val_loss: 0.2704 - val_acc: 0.9246\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2661 - acc: 0.9258 - val_loss: 0.2702 - val_acc: 0.9248\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2660 - acc: 0.9260 - val_loss: 0.2703 - val_acc: 0.9247\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2658 - acc: 0.9262 - val_loss: 0.2702 - val_acc: 0.9249\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2657 - acc: 0.9258 - val_loss: 0.2702 - val_acc: 0.9247\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2656 - acc: 0.9259 - val_loss: 0.2702 - val_acc: 0.9253\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2655 - acc: 0.9261 - val_loss: 0.2701 - val_acc: 0.9248\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2654 - acc: 0.9260 - val_loss: 0.2700 - val_acc: 0.9253\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2652 - acc: 0.9262 - val_loss: 0.2699 - val_acc: 0.9249\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2650 - acc: 0.9262 - val_loss: 0.2700 - val_acc: 0.9253\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2650 - acc: 0.9264 - val_loss: 0.2699 - val_acc: 0.9254\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2648 - acc: 0.9262 - val_loss: 0.2698 - val_acc: 0.9253\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2648 - acc: 0.9262 - val_loss: 0.2699 - val_acc: 0.9256\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2646 - acc: 0.9264 - val_loss: 0.2697 - val_acc: 0.9251\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2645 - acc: 0.9264 - val_loss: 0.2697 - val_acc: 0.9253\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2643 - acc: 0.9259 - val_loss: 0.2695 - val_acc: 0.9257\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.2642 - acc: 0.9267 - val_loss: 0.2695 - val_acc: 0.9255\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2641 - acc: 0.9266 - val_loss: 0.2694 - val_acc: 0.9258\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2640 - acc: 0.9262 - val_loss: 0.2695 - val_acc: 0.9259\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2639 - acc: 0.9264 - val_loss: 0.2694 - val_acc: 0.9253\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2637 - acc: 0.9265 - val_loss: 0.2692 - val_acc: 0.9255\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2636 - acc: 0.9268 - val_loss: 0.2695 - val_acc: 0.9255\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2635 - acc: 0.9266 - val_loss: 0.2693 - val_acc: 0.9258\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2635 - acc: 0.9265 - val_loss: 0.2692 - val_acc: 0.9254\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2633 - acc: 0.9267 - val_loss: 0.2690 - val_acc: 0.9261\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2632 - acc: 0.9267 - val_loss: 0.2690 - val_acc: 0.9261\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2631 - acc: 0.9269 - val_loss: 0.2691 - val_acc: 0.9253\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2630 - acc: 0.9267 - val_loss: 0.2690 - val_acc: 0.9253\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2629 - acc: 0.9267 - val_loss: 0.2689 - val_acc: 0.9257\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2628 - acc: 0.9268 - val_loss: 0.2687 - val_acc: 0.9260\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2627 - acc: 0.9270 - val_loss: 0.2689 - val_acc: 0.9255\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2626 - acc: 0.9270 - val_loss: 0.2688 - val_acc: 0.9257\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2624 - acc: 0.9267 - val_loss: 0.2688 - val_acc: 0.9257\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2623 - acc: 0.9266 - val_loss: 0.2689 - val_acc: 0.9256\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2623 - acc: 0.9267 - val_loss: 0.2687 - val_acc: 0.9257\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2621 - acc: 0.9270 - val_loss: 0.2687 - val_acc: 0.9267\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2621 - acc: 0.9269 - val_loss: 0.2686 - val_acc: 0.9259\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2619 - acc: 0.9270 - val_loss: 0.2686 - val_acc: 0.9263\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2618 - acc: 0.9271 - val_loss: 0.2686 - val_acc: 0.9262\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2617 - acc: 0.9270 - val_loss: 0.2685 - val_acc: 0.9261\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2616 - acc: 0.9268 - val_loss: 0.2685 - val_acc: 0.9262\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2615 - acc: 0.9270 - val_loss: 0.2685 - val_acc: 0.9258\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2615 - acc: 0.9269 - val_loss: 0.2684 - val_acc: 0.9263\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2613 - acc: 0.9272 - val_loss: 0.2684 - val_acc: 0.9267\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,verbose=VERBOSE,validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 20us/step\n",
      "('Test score:', 0.27130199307501318)\n",
      "('Test accuracy', 0.92320000000000002)\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(X_test,y_test,verbose=VERBOSE)\n",
    "print(\"Test score:\",score[0])\n",
    "print('Test accuracy',score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "Train acc:  92.73%\n",
    "\n",
    "val acc:    92.63%\n",
    "\n",
    "Test acc :  92.37%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
